{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Data",
   "id": "58f0e5b08e2269d"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-19T09:45:46.565320Z",
     "start_time": "2025-09-19T09:45:46.556557Z"
    }
   },
   "cell_type": "code",
   "source": "import pandas as pd",
   "id": "5fc8afd11d8d2b74",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T09:45:57.042582Z",
     "start_time": "2025-09-19T09:45:49.301593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_df = pd.read_csv('../data/Easier Dataset.csv')\n",
    "\n",
    "# represent each planetary system as a matrix\n",
    "matrices = [\n",
    "    group.drop(columns=['system_number']).to_numpy()\n",
    "    for _, group in data_df.groupby('system_number')\n",
    "]"
   ],
   "id": "e7c5c5c4980af868",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LSTM-VAE",
   "id": "bd417540f0e1cf15"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T09:57:41.554354Z",
     "start_time": "2025-09-19T09:57:41.546472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "import torch.nn.functional as F"
   ],
   "id": "2f775d96390a2223",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T09:57:45.424841Z",
     "start_time": "2025-09-19T09:57:45.416040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PlanetarySystemsDataset(Dataset):\n",
    "    def __init__(self, matrices):\n",
    "        self.matrices = matrices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.matrices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.matrices[idx], dtype=torch.float32)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Normalize each sequence before padding\n",
    "    normed = [nn.LayerNorm(seq.shape[1])(seq) for seq in batch]\n",
    "    lengths = [seq.shape[0] for seq in normed]\n",
    "    padded = pad_sequence(normed, batch_first=True)\n",
    "    return padded, torch.tensor(lengths, dtype=torch.long)"
   ],
   "id": "832603db407eccc0",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T09:57:54.918154Z",
     "start_time": "2025-09-19T09:57:54.903550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTMVAEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, latent_dim=20):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=2, batch_first=True)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.length_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.length_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.LayerNorm(hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 4, 1)\n",
    "        )\n",
    "\n",
    "        # MLP to map z to length\n",
    "        self.z_to_length = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.LayerNorm(hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 4, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        _, (h_n, _) = self.lstm(packed)\n",
    "        h = h_n[-1]\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        normed_h = self.length_norm(h)\n",
    "        length_pred = self.length_head(normed_h).squeeze(-1)\n",
    "        return mu, logvar, length_pred\n",
    "\n",
    "    def predict_length_from_z(self, z):\n",
    "        return self.z_to_length(z).squeeze(-1)\n",
    "\n",
    "class LSTMVADecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(latent_dim, hidden_dim, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, z, seq_len):\n",
    "        # z: (batch, latent_dim)\n",
    "        repeated = z.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        out, _ = self.lstm(repeated)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ],
   "id": "af11eb0af8881e11",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T10:01:09.016085Z",
     "start_time": "2025-09-19T10:01:08.997159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare dataset and dataloader\n",
    "dataset = PlanetarySystemsDataset(matrices)\n",
    "BATCH_SIZE = 64\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "ENCODING_SIZE = 20\n",
    "LSTM_HIDDEN_SIZE = 128\n",
    "input_dim = matrices[0].shape[1]\n",
    "\n",
    "# Instantiate models\n",
    "encoder = LSTMVAEncoder(input_dim=input_dim, hidden_dim=LSTM_HIDDEN_SIZE, latent_dim=ENCODING_SIZE)\n",
    "decoder = LSTMVADecoder(output_dim=input_dim, hidden_dim=LSTM_HIDDEN_SIZE, latent_dim=input_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-3)"
   ],
   "id": "9e759abf59295f08",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T10:05:01.298955Z",
     "start_time": "2025-09-19T10:01:46.283695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_dim = matrices[0].shape[1]\n",
    "HIDDEN_DIM = 128\n",
    "LATENT_DIM = 20\n",
    "\n",
    "encoder = LSTMVAEncoder(input_dim, HIDDEN_DIM, LATENT_DIM).to(device)\n",
    "decoder = LSTMVADecoder(LATENT_DIM, HIDDEN_DIM, input_dim).to(device)\n",
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-3)\n",
    "criterion = nn.MSELoss(reduction='sum')  # sum for proper KL scaling\n",
    "\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    total_loss = 0\n",
    "    for padded, lengths in loader:\n",
    "        padded = padded.to(device)\n",
    "        batch_size = padded.size(0)\n",
    "        optimizer.zero_grad()\n",
    "        mu, logvar, length_pred = encoder(padded, lengths)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "\n",
    "        length_pred_from_z = encoder.predict_length_from_z(z)\n",
    "        length_loss_z = F.mse_loss(length_pred_from_z, lengths.float().to(device))\n",
    "        length_loss = F.mse_loss(length_pred, lengths.float().to(device))\n",
    "\n",
    "        recon = decoder(z, padded.shape[1])\n",
    "        # Mask out padded values for loss\n",
    "        mask = torch.arange(padded.shape[1])[None, :].to(device) < lengths[:, None].to(device)\n",
    "        recon_loss = ((recon - padded) ** 2).sum(dim=2)\n",
    "        recon_loss = (recon_loss * mask).sum() / mask.sum()\n",
    "        # KL divergence\n",
    "        kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / batch_size\n",
    "\n",
    "        loss = recon_loss + 1e-3 * kld + length_loss + length_loss_z\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ],
   "id": "20b89fc6c4019bd1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 98692.3960\n",
      "Epoch 2, Loss: 10319.4348\n",
      "Epoch 3, Loss: 555.8332\n",
      "Epoch 4, Loss: 291.1423\n",
      "Epoch 5, Loss: 248.4918\n",
      "Epoch 6, Loss: 233.7783\n",
      "Epoch 7, Loss: 216.9952\n",
      "Epoch 8, Loss: 205.2572\n",
      "Epoch 9, Loss: 196.1170\n",
      "Epoch 10, Loss: 188.3839\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T10:48:40.069717Z",
     "start_time": "2025-09-19T10:48:39.880036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(1, LATENT_DIM).to(device)\n",
    "    length_pred = encoder.predict_length_from_z(z)\n",
    "    seq_len = int(torch.clamp(length_pred.round(), min=1, max=50).item())\n",
    "    generated = decoder(z, seq_len)\n",
    "    generated = generated.cpu().numpy()"
   ],
   "id": "9e10c7b06fd4f64a",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T10:48:48.667160Z",
     "start_time": "2025-09-19T10:48:48.658146Z"
    }
   },
   "cell_type": "code",
   "source": "generated",
   "id": "fcb3d11371c5d157",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.7783725 ,  1.2636299 , -0.48817095],\n",
       "        [-0.6269555 ,  1.2938346 , -0.69207555]]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
